(1) compute cross entropy loss : (also known as log loss. It is basically a  loss function.) 
    It measures the performance of a classification model whose output is a probability value 
    between 0 and 1. The loss increases as the predicted probability diverges from the 
    actual label.

(2) 2-class classification neural network :  2-class classification neural network, 
    also known as a binary classification neural network, is designed to categorize inputs 
    into one of two distinct classes.

(3) OverFitting

(4) What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?
    Play with the learning_rate. What happens?
    What if we change the dataset? (See part 7 below!)